# Algorithme des k plus proches voisins

## Apprentissage automatique (*machine learning *)

L'apprentissage automatique, ou *machine learning* en anglais, est un domaine cl√© de l'**intelligence artificielle**. Il repose sur des m√©thodes math√©matiques et statistiques qui permettent aux ordinateurs **d'apprendre √† partir de donn√©es** : autrement dit, √† am√©liorer leurs performances dans l'ex√©cution de certaines t√¢ches, sans que chaque √©tape soit explicitement programm√©e.

L'apprentissage automatique comporte g√©n√©ralement deux phases. Une premi√®re phase d'**apprentissage (ou entra√Ænement)** consiste √† analyser un ensemble de donn√©es connues (donn√©es d'entra√Ænement) afin de construire un mod√®le. Une fois ce mod√®le d√©termin√©, la seconde phase de **production (ou d'inf√©rence)** consiste √† lui soumettre de nouvelles donn√©es pour obtenir une pr√©diction, une classification ou une d√©cision.


On distingue trois principaux types d'apprentissage automatique :

|Type d'apprentissage|Description|Exemples|
|:-|:-|:-|
|üß© L'apprentissage supervis√©|Les donn√©es d'entra√Ænement incluent les r√©ponses attendues|Pr√©diction m√©t√©o, reconnaissance d'images|
|üîç L'apprentissage non supervis√©|Les donn√©es sont brutes, l'algorithme doit trouver des structures cach√©es|Regroupement de clients, segmentation marketing|
|üéÆ L'apprentissage par renforcement|L'algorithme apprend en interagissant avec son environnement, il re√ßoit des r√©compenses ou p√©nalit√©s|Jeux d'√©checs, optimisation robotique|


![L'apprentissage automatique en AI et les types d'apprentissage](assets/3-kppv-apprentissage-automatique-light-mode.png#only-light)
![L'apprentissage automatique en AI et les types d'apprentissage](assets/3-kppv-apprentissage-automatique-dark-mode.png#only-dark)


L'algorithme des k plus proches voisins (KPPV) est un algorithme d'**apprentissage automatique supervis√©**. 


Il existe d'autres formes d'apprentissage automatique, par exemple les algorithmes d'apprentissage profond (ou *deep learning*) qui s'appuient sur des r√©seaux de neurones artificiels √† plusieurs couches (d'o√π le nom ¬´ profond ¬ª), tels que les grands mod√®les de langages (ou LLM pour *large language models*) : ChatGPT, Gemini, Le Chat Mistral, etc.


## Principe de l'algorithme


!!! abstract "Cours" 
    L'algorithme des k plus proches voisins (KPPV) ou *k-nearest neigbors* (KNN) permet de r√©soudre des probl√®mes de **r√©gression** (estimer la valeur d'une nouvelle donn√©e) ou de **classification** (d√©terminer √† quelle classe appartient une nouvelle donn√©e) √† partir des k plus proches parmi des **donn√©es d'entra√Ænement**. La proximit√© est souvent mesur√©e √† l'aide de la **distance euclidienne**[^3.1].
 


[^3.1]: D'autres distances existent, par exemple la distance de Manhattan calcul√©e en utilisant les d√©placements horizontaux et verticaux.

Prenons un exemple simple de classification. Les bonbons rouges d'un c√©l√®bre confiseur appartiennent √† deux **classes** diff√©rentes, certains sont au go√ªt fraise, d'autres sont au go√ªt framboise. On veut d√©terminer la classe d'un bonbon rouge inconnu. Pour nous aider, on dispose de 5 bonbons de chaque classe, ce sont les **donn√©es d'entra√Ænement**, dont on a mesur√© le poids et la taille. Il est tr√®s difficile de les diff√©rencier √† vue d'oeil mais Les bonbons au go√ªt fraise sont souvent un peu plus grands et plus l√©gers que ceux au go√ªt framboise.  


On a mesur√© les valeurs suivantes sur les donn√©es d'entra√Ænement :


![Donn√©es d'entra√Ænement contenant 5 bonbons fraises et 6 bonbons framboises ](assets/3-kppv-donnees-d-entrainement.png){width=50% align=right}


|poids (g)|taille (mm)|classe|
|:-:|:-:|:-|
| 5 | 7 | fraise |
| 6 | 8 | fraise |
| 6 | 6 | fraise |
| 7 | 5 | framboise |
| 7 | 9 | fraise |
| 8 | 8 | fraise |
| 9 | 6 | framboise |
| 9 | 8 | framboise |
| 10 | 5 | framboise |
| 11 | 7 | framboise |


On veut d√©terminer la classe d'une nouvelle donn√©e: un bonbon rouge inconnu. On sait qu'il p√®se 8 g et mesure 7 mm mais est-ce un bonbon au go√ªt fraise ou au go√ªt framboise ?  

üîç √âtape 1 : calcul des distances

La distance euclidienne entre deux points de coordonn√©es $(x_1, y_1)$ et $(x_2,y_2)$ dans le plan[^3.2] est donn√©e par la formule :  $d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$ .

Calculons les distances entre chaque donn√©e d'entra√Ænement et cette nouvelle donn√©e :

[^3.2]: avec un rep√®re orthonorm√©.

![Calcul des distances](assets/3-kppv-calcul-des-distances.png){width=50% align=right}

|poids (g)|taille (mm)|classe|distance|
|:-:|:-:|:-|:-:|
| 5 | 7 | fraise | 3.0 |
| 6 | 8 | fraise | 2.24 |
| 6 | 6 | fraise | 2.24 |
| 7 | 5 | framboise | 2.24 |
| 7 | 9 | fraise | 2.24 |
| 8 | 8 | fraise | 1.0 |
| 9 | 6 | framboise | 1.41 |
| 9 | 8 | framboise | 1.41 |
| 10 | 5 | framboise | 2.83 |
| 11 | 7 | framboise | 3.0 |



üé® √âtape 2 : vote des k voisins

![Classement avec les 3 plus proches voisins](assets/3-kppv-choix-k-1-3.png){width=50% align=right}

L'approche la plus simple consiste √† utiliser la classe du voisin le plus proche parmi les donn√©es d'entra√Ænement, c'est-√†-dire k = 1. C'est le bonbon qui p√®se 8 g et mesure 8 mm qui se trouve √† une distance de 1 de la nouvelle donn√©e: il est au go√ªt fraise. 

üëâ Le bonbon inconnu est de la m√™me classe que son voisin le plus proche, il est donc au go√ªt fraise.


Mais on peut aussi prendre une autre approche qui consiste √† prendre compte plusieurs voisins, par exemple les 3 voisins les plus proches, c'est-√†-dire k = 3. Parmi les 3 bonbons les plus proches, un est au go√ªt fraise et deux au go√ªt framboise.  

üëâ Le bonbon inconnu est de la classe majoritaire de ses 3 voisins les plus proches, il est donc au go√ªt framboise.

Comme on peut le voir, le choix de la valeur de k utilis√©e dans l'algorithme est d√©terminant sur le r√©sultat obtenu ! La phase d'apprentissage permet de choisir la meilleure valeur de k[^3.3]. On choisit en principe un nombre impair pour √©viter les cas d'√©galit√© entre plusieurs classes.

[^3.3] une m√©thode classique est la validation crois√©e (*cross validation*).

Dans cette exemple, nous avons √©tudi√© un probl√®me de classification. Dans le cas d'un probl√®me de r√©gression, l'approche est la m√™me en calculant la valeur moyenne des k plus proches voisins plut√¥t que la classe majoritaire.

## Co√ªt de l'algorithme
Etudions le c√¥ut de l'algorithme des k plus proches voisins. Pour $n$ donn√©es d'entra√Ænement, l'algorithme consiste √† parcourir chaque donn√©e pour calculer sa distance avec la donn√©e inconnue. Le co√ªt est donc **lin√©aire en $O(n)$**. 

Le tri du tableau des distance rajoute ici une compl√©xit√© suppl√©mentaire, en $O(n^2)$ pour les tris les moins efficaces. N√©anmoins on peut tr√®s bien se passer de ce tri pour optimiser l'algorithme et enregistrer directement les classes ou les valeurs des k plus proches voisins pendant le calcul des distances.

## Exemple : Iris de Fisher

Le jeu de donn√©es Iris connu aussi sous le nom de Iris de Fisher est un jeu de donn√©es multivari√©es pr√©sent√© en 1936 par Ronald Fisher dans son papier ¬´¬†The use of multiple measurements in taxonomic problems¬†¬ª comme un exemple d'application de l'analyse discriminante lin√©aire. [‚Ä¶]

Le jeu de donn√©es comprend 50 √©chantillons de chacune des trois esp√®ces d'iris (Iris setosa, Iris virginica et Iris versicolor).  Quatre caract√©ristiques ont √©t√© mesur√©es √† partir de chaque √©chantillon : la longueur et la largeur des s√©pales et des p√©tales, en centim√®tres. Sur la base de la combinaison de ces quatre variables, Fisher a √©labor√© un mod√®le d'analyse discriminante lin√©aire permettant de distinguer les esp√®ces les unes des autres.

||||
|:-:|:-:|:-:|
|![Iris setosa](assets/500px-Kosaciec_szczecinkowaty_Iris_setosa.jpg)Iris setosa|![Iris versicolor](assets/960px-Iris_versicolor_3.jpg)Iris versicolor|![Iris setosa](assets/960px-Iris_virginica.jpg)Iris virginica|

Bas√© sur le mod√®le d'analyse lin√©aire discriminante de Fisher, ce jeu de donn√©es est devenu un cas typique pour de nombreuses techniques de classification automatique en apprentissage automatique (*machine learning*).

Source¬†: [https://fr.wikipedia.org/wiki/Iris_de_Fisher](https://fr.wikipedia.org/wiki/Iris_de_Fisher)

!!! question "Exercice corrig√©" 

    1.  Copier le fichier [¬´¬†iris.csv¬†¬ª](assets/iris.csv) dans vos documents et visualiser avec le blocnote son contenu. Quel est le caract√®re utilis√© pour s√©parer les donn√©es dans le fichier ? Quels sont les descripteurs¬†des donn√©es ?

    2.  Cr√©er un nouveau programme Python enregistr√© dans le m√™me r√©pertoire que le fichier "iris.csv" puis importer les donn√©es contenues dans le fichier avec le code suivant en renseignant le caract√®re de s√©paration des donn√©es (param√®tre `delimiter`) :

        ``` py
        import csv

        with open('iris.csv', 'r') as f:
            iris = list(csv.DictReader(f, delimiter='...')
        ```

    3.  Quel est le type Python de la variable `iris` ?

        a) un dictionnaire		b) un dictionnaire de tableaux	c) un tableau de dictionnaires	d) un tableau de tableaux

    4.  Ajouter au programmme une fonction `distance_euclidienne` qui prend en param√®tre `iris1` et `iris2`, deux √©l√©ments de la variable `iris`, et qui renvoie la distance euclidienne entre les valeurs `('largeur_petale', 'longueur_petale')`. 

        Aide¬†: la fonction `sqrt` du module `math` renvoie la racine carr√©e d'un nombre. 
        
        Attention au type de `'largeur_petale'` et `'longueur_petale'`¬†! 

        Exemple¬†: 
        ``` py
        >>> distance_euclidienne(iris[0], iris[3])
        0.10000000000000009
        ```

    5.  √âcrire une fonction `trier_par_distance` qui prend en param√®tre¬†`inconnu`, un dictionnaire dont les cl√©s sont 'largeur_petale' et 'longueur_petale' et les valeurs celles d'un iris inconnu, et renvoie un tableau de dictionnaires avec les cl√©s `'id'`, `'distance'` et `'espece'` pour chaque iris de la variable `iris`, tri√©s par distances croissantes.

        Aide¬†: L'instruction `distances.sort(key=lambda x: x["distance"])` permet de trier le tableau de dictionnaires `distances` contenant une cl√© `"distance"` par ordre croissant de cette cl√©. 

        Exemple¬†: 
        ``` py
        >>> iris_inconnnu = {'largeur_petale': '0.3', 'longueur_petale': '2.1',}
        >>> trier_par_distance(iris_inconnnu)
        [[{'id': '61', 'distance': 0.7, 'espece': 'Iris-versicolor'}, {'id': '80', 'distance': 0.7, 'espece': 'Iris-versicolor'}, {'id': '58', 'distance': 0.7280109889280518, 'espece': 'Iris-versicolor'}...
        ```

    6.  √âcrire la fonction `kppv` qui prend en param√®tres¬†:
        - `inconnu`, un dictionnaire contenant un iris inconnu
        - `k` un entier
        et renvoie un dictionnaire contenant le nombre d'iris de chaque esp√®ce parmi les k plus proches voisins de l'iris inconnu.

        Exemple¬†:
        ``` py
        >>> kppv(iris_inconnnu, k=9)
        {'Iris-setosa': 0, 'Iris-versicolor': 9, 'Iris-virginica': 0}
        ```

        


    

??? Success "R√©ponse"
    1.  Les valeurs sont s√©par√©es par des  virgules , les descripteurs sont¬†: id,longueur_sepale, largeur_sepale, longueur_petale, largeur_petale, espece.

    2.  

        ``` py
        import csv

        with open('iris.csv', 'r') as f:
            iris = list(csv.DictReader(f, delimiter=',')
        ```

    3.  un tableau de dictionnaires

    4.  

        ``` py
        from math import sqrt

        def distance_euclidienne(iris1, iris2):
            x1, y1 = float(iris1['largeur_petale']), float(iris1['longueur_petale'])
            x2, y2 = float(iris2['largeur_petale']), float(iris2['longueur_petale'])
            return sqrt((x2 - x1)**2 + (y2 ‚Äì y1)**2))
        ```
    
    5.  

        ``` py
        def trier_par_distance(inconnu):
            """ dict -> list[dict]
            Renvoie un tableau de dictionnaires {'id':_, 'distance':_, 'espece':_}
            pour chaque iris d'entrainement, tri√©s par distances croissantes
            """
            distances = []
            for i in iris:
                distances.append({'id': i['id'],
                                  'distance': distance_euclidienne(i, inconnu),
                                  'espece':  i['espece']})
            distances.sort(key=lambda x: x["distance"])
            return distances
        ```

    6.  

        ``` py
        def kppv(inconnu, k):
            """ dict, int -> dict
            Renvoie un dictionnaire contenant le nombre d'iris de chaque esp√®ce 
            parmi les k plus proches voisins de l'iris inconnu 
            """
            especes = {'Iris-setosa':0, 'Iris-versicolor':0, 'Iris-virginica':0}
            plus_proches = trier_par_distance(inconnu)[:k]
            for pp in plus_proches:
                especes[pp['espece']] += 1
            return especes
        ```


